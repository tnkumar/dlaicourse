*** Various exercises
Course 1 Week 1 - House price with number of bedrooms
Course 1 Week 2 Lab 1- fashion_mnist - with various variations
Course 1 Week 2 Exercise 2 - mnist data - reach 99% accuracy



*** Imports

import tensorflow as tf
from tensorflow import keras

import numpy as np
np.set_printoptions(linewidth=200)

import matplotlib.pyplot as plt

*** Checking version number of tensorflow
print (tf.__version__)

*** Loading images from keras datasets                                                        # Course 1 Week 2 
fashion_mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data()

mnist = tf.keras.datasets.mnist                                                               # Course 1 Week 2 Exercise 2

*** Show image using matplotlib
plt.imshow (training_images[2])

*** Normalizing
training_images = training_images / 255.0

*** Specifying keras models
model=tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])                     # Course 1 Week 1 Lab 1

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),                                # Course 1 Week 2 - fashion_mnist - 28x28 grayscale
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),        # was tested with 1024 neurons instead of 128
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
                                    
model = tf.keras.models.Sequential([                                                          # Course 1 Week 2 Exercise 2 - mnist handwriting
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])   

*** Conv layer tf documentation
tf.keras.layers.Conv2D(
    filters, kernel_size, strides=(1, 1), padding='valid',
    data_format=None, dilation_rate=(1, 1), groups=1, activation=None,
    use_bias=True, kernel_initializer='glorot_uniform',
    bias_initializer='zeros', kernel_regularizer=None,
    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,
    bias_constraint=None, **kwargs
)

*** Max pool layer tf documentation
tf.keras.layers.MaxPool2D(
    pool_size=(2, 2), strides=None, padding='valid', data_format=None,
    **kwargs
)
                                    
*** Compiling a model
model.compile(optimizer = 'sgd', loss = 'mean_squared_error')                                  # Course 1 Week 1
model.compile(optimizer = tf.optimizers.Adam(),                                                # Course 1 Week 2
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])
              
model.compile(optimizer='adam',                                                                # Course 1 Week 2 Exercise 2
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])              
              
*** params in compile function -> optimizer, loss, metrics, loss_weights, weighted metrics, run_eagerly, steps_per_execution
                                                                                               # uses the first two or 3 at most
*** Optimizers for models -> sgd, Adam, RMSprop - others from keras -> Adadelta, Adagrad, Adamax, Ftrl, Nadam
                                                                                               # seen the first 3 being used
*** Adagrad automatically changes the learning rate
*** Adadelta and RMSprop are extensions to Adagrad
*** Adam has adaptive learning rates and also has the concept of momentum
*** If input data is sparse, best resut from adaptive method, Adam can be a good choice

*** Combinations of optimizers and loss functions - sgd, mean_squared_error

*** Fitting a model
model.fit(training_images, training_labels, epochs=5)
model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])                   # When callbacks defined, add to model.fit

*** Evaluate a model
model.evaluate(test_images, test_labels)

*** Use model to classify
classifications = model.predict(test_images)
print(classifications[0])

*** Defining class myCallback and instantiating an object callbacks
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('loss')<0.4):
      print("\nReached 60% accuracy so cancelling training!")
      self.model.stop_training = True

callbacks = myCallback()

*** Course 1 Week 2 variations fashion_mnist dataset                                         # Course 1 Week 2 variations 
128 neurons in intermediate Dese layer               - loss started at 0.6 and became .28 in 5 epochs   accuraacy 0.78 to .89
1024                                                                   0.3            .02    5                    0.90 to .99
512 without normalization                                              7              .26    5                    0.86 to .94
512 and 256 Adding extra layer - not much improvement                  0.31           .02    5                    0.90 to .99
128 neurons                                                                                  30                   0.87 to .9987
By removing flatten layer, the shape will not be correct and there will be an error
If you have less neurons in the last layer than the number of classes, there will be an error
Adding metrics in compile, prints out accuracy as well
              
